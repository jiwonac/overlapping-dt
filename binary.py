import random
import pprint
import math
from datagenerator import data_generator

def argmin(iterable):
    return min(enumerate(iterable), key=lambda x: x[1])[0]

def argmax(iterable):
    return max(enumerate(iterable), key=lambda x: x[1])[0]

def binary_dt(dataset, query, type):
	"""
	Implements the known binary DT algorithm. 
	Parameters:
		dataset: a properly formatted dataset generated by data_generator
		query: list of count requirements for each group
		type: a string identifying the data source choice rule used
			"orig": original (data source with maximum % of minority group)
			"1-p/1+p": argmax{[1 - P(overlap)] / [1 + P(overlap)]}
			"random": choose data sources randomly
			"ratio": choose more common groups rarely and rare groups more often
	"""
	# Things to keep track of
	num_sources = dataset["num_sources"]
	unified_set = set()
	expected_from_sources = [None] + [0 for x in range(num_sources + 1)]
	total_cost = 0.0
	total_sampling_attempts = 0
	# Find the minority group overall
	num_groups = dataset["num_groups"]
	group_counts = [0.0 for x in range(num_groups)]
	group_dists = dataset["group_dists"]
	for source in range(1, num_sources  + 1):
		for group in range(num_groups):
			group_counts[group] += group_dists[source][group]
	minority_group = argmin(group_counts)
	# Keep on adding data points to unified set
	while(all(query)): # While at least one count requirement is not 0
		total_sampling_attempts += 1
		# Choose the optimal data source
		data_source = None
		if (type == "orig"):
			minority_group_ratios = [float('-inf')] + [group_dists[source][minority_group]
				/ (len(dataset["sources"][source]) * dataset["costs"][source]) for source in range(1, num_sources + 1)]
			data_source = argmax(minority_group_ratios)
		elif (type == "1-p/1+p"):
			expected_overlap_prob = [None] + [expected_from_sources[source] 
				/ len(dataset["sources"][source]) for source in range(1, num_sources + 1)]
			#print(expected_overlap_prob)
			minority_group_ratios = [float('-inf')] + [
				(group_dists[source][minority_group] / len(dataset["sources"][source])) 
				* math.pow(((1 - expected_overlap_prob[source]) / (1 + expected_overlap_prob[source])), 1)
				* (1 / dataset["costs"][source]) for source in range(1, num_sources + 1)]
			data_source = argmax(minority_group_ratios)
		elif (type == "random"):
			data_source = random.randrange(1, num_sources)
		elif (type == "ratio"):
			pass
		# Sample a data point from chosen source
		new_point = random.choice(dataset["sources"][data_source])
		new_point_val = new_point["val"]
		total_cost += dataset["costs"][data_source] # Pay cost			
		# Duplicate check & branch
		if (new_point_val not in unified_set): # Not Overlap
			unified_set.add(new_point_val)
			# Update expected number of samples
			for source in range(1, num_sources + 1):
				if (source == data_source):
					expected_from_sources[data_source] += 1.0
				else:
					mytuple = ()
					for s in range(1, num_sources + 1):
						if (s == data_source or s == source):
							mytuple += (s,)
						else:
							mytuple += (-s,)
					expected_from_sources[data_source] += (dataset["distribution"][mytuple]) / (len(dataset["sources"][source]))
			# Update query
			new_group = new_point["group"]
			query[new_group] -= 1
	return unified_set, total_cost, total_sampling_attempts


if __name__ == "__main__":
	# Generate dataset
	num_sources = 2
	distribution = {(1, -2): 70, (-1, 2): 70, (1, 2): 30}
	costs = [None, 1.0, 1.0]
	num_groups = 2
	group_dists = [None, (60, 40), (70, 30)]
	dataset = data_generator(num_sources, distribution, costs, num_groups, group_dists)
	#pp = pprint.PrettyPrinter(width=41, compact=True)
	#pp.pprint(dataset)
	total_costs = [0.0, 0.0, 0.0]
	n = 40
	experiments = 3000
	for i in range(experiments):
		# Start testing!
		query = [n, n]
		orig_unified_set, orig_total_cost, orig_total_sampling_attempts = binary_dt(dataset, query, "orig")
		#print("orig: ", str(orig_total_cost), str(orig_total_sampling_attempts))
		total_costs[0] += orig_total_cost

		query = [n, n]
		new_unified_set, new_total_cost, new_total_sampling_attempts = binary_dt(dataset, query, "1-p/1+p")
		#print("new: ", str(new_total_cost), str(new_total_sampling_attempts))
		total_costs[1] += new_total_cost

		query = [n, n]
		random_unified_set, random_total_cost, random_total_sampling_attempts = binary_dt(dataset, query, "random")
		#print("random: ", str(random_total_cost), str(random_total_sampling_attempts))
		total_costs[2] += random_total_cost
	costs = [x / experiments for x in total_costs]
	print(total_costs)
